{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNXlRmEyjqJe2l8n+J47o+T"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLabiJQbovRp",
        "outputId": "3804e8dc-8f19-4862-e6e9-af482902a717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced FLAVA Defect Segmentation Training with Multi-scale Features and Advanced Evaluation\n",
        "# Compatible with Google Colab environment\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import Resize\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import FlavaModel, FlavaProcessor\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.manifold import TSNE\n",
        "from scipy.stats import ttest_ind, pearsonr\n",
        "\n",
        "# ==== CONFIG ====\n",
        "class Config:\n",
        "    base_model_path = \"/content/drive/MyDrive/flava_finetuned\"\n",
        "    data_path = \"/content/drive/MyDrive/Data12 class segmentation\"\n",
        "    save_path = \"/content/drive/MyDrive/new_flava/attention_seg_head_enhanced\"\n",
        "    debug_dir = os.path.join(save_path, \"debug\")\n",
        "    plots_dir = os.path.join(save_path, \"plots\")\n",
        "    metrics_dir = os.path.join(save_path, \"metrics\")\n",
        "    analysis_dir = os.path.join(save_path, \"advanced_analysis\")\n",
        "    gradcam_dir = os.path.join(save_path, \"gradcam\")\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    batch_size = 4\n",
        "    num_epochs = 10\n",
        "    lr = 2e-5\n",
        "    patch_grid = 14\n",
        "    mask_size = (14, 14)\n",
        "    use_multi_scale = True  # Use multi-scale features from multiple transformer layers\n",
        "    use_advanced_loss = True  # Use FocalDiceLoss instead of BCE\n",
        "    test_split = 0.2  # Portion of data to use for validation\n",
        "    synthetic_multi_scale = True  # Use synthetic multi-scale features if real ones not available\n",
        "\n",
        "# Create necessary directories\n",
        "for directory in [Config.save_path, Config.debug_dir, Config.plots_dir, Config.metrics_dir, Config.analysis_dir, Config.gradcam_dir]:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "print(f\"Using device: {Config.device}\")\n",
        "\n",
        "# ==== DATASET ====\n",
        "class MaskedDataset(Dataset):\n",
        "    def __init__(self, data_dir):\n",
        "        self.imgs, self.masks = [], []\n",
        "        self.img_transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "        self.mask_resize = Resize(Config.mask_size)\n",
        "\n",
        "        for cls in os.listdir(data_dir):\n",
        "            p = os.path.join(data_dir, cls)\n",
        "            if not os.path.isdir(p): continue\n",
        "            for f in os.listdir(p):\n",
        "                if f.endswith(\".json\"):\n",
        "                    img = os.path.join(p, f.replace(\".json\", \".jpg\"))\n",
        "                    jsn = os.path.join(p, f)\n",
        "                    if os.path.exists(img):\n",
        "                        self.imgs.append(img)\n",
        "                        self.masks.append(jsn)\n",
        "\n",
        "        print(f\"Found {len(self.imgs)} images with masks\")\n",
        "\n",
        "    def __len__(self): return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        img = Image.open(self.imgs[i]).convert(\"RGB\")\n",
        "        img_tensor = self.img_transform(img)\n",
        "\n",
        "        mask_arr = np.zeros((640, 640), dtype=np.uint8)\n",
        "        with open(self.masks[i]) as f:\n",
        "            data = json.load(f)\n",
        "            for ann in data.get('annotations', []):\n",
        "                x, y, w, h = map(int, ann['bbox'])\n",
        "                x2, y2 = min(x+w, 640), min(y+h, 640)\n",
        "                mask_arr[y:y2, x:x2] = 1\n",
        "        mask = self.mask_resize(Image.fromarray(mask_arr * 255))\n",
        "        mask_tensor = transforms.ToTensor()(mask).float().squeeze(0)\n",
        "\n",
        "        return img_tensor, mask_tensor, self.imgs[i]\n",
        "\n",
        "# ==== LOSS FUNCTION ====\n",
        "class FocalDiceLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.5, gamma=2.0, beta=0.5):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha  # Focal loss weight\n",
        "        self.gamma = gamma  # Focal loss focusing parameter\n",
        "        self.beta = beta    # Weight between BCE and Dice loss\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        # Binary cross entropy with logits\n",
        "        bce_loss = nn.functional.binary_cross_entropy_with_logits(\n",
        "            inputs, targets, reduction='none'\n",
        "        )\n",
        "\n",
        "        # Focal term\n",
        "        probs = torch.sigmoid(inputs)\n",
        "        pt = torch.where(targets == 1, probs, 1-probs)\n",
        "        focal_weight = (1-pt) ** self.gamma\n",
        "        focal_loss = focal_weight * bce_loss\n",
        "\n",
        "        # Dice loss\n",
        "        inputs_sigmoid = torch.sigmoid(inputs)\n",
        "        intersection = (inputs_sigmoid * targets).sum((1,2))\n",
        "        union = (inputs_sigmoid + targets).sum((1,2))\n",
        "        dice_loss = 1 - (2. * intersection + 1e-6) / (union + 1e-6)\n",
        "\n",
        "        # Combine losses\n",
        "        combined_loss = self.beta * focal_loss.mean() + (1-self.beta) * dice_loss.mean()\n",
        "        return combined_loss\n",
        "\n",
        "# ==== SEGMENTATION HEAD ====\n",
        "class FLAVASegmenter(nn.Module):\n",
        "    def __init__(self, base_model_path):\n",
        "        super().__init__()\n",
        "        self.model = FlavaModel.from_pretrained(base_model_path)\n",
        "        # Extract features from multiple transformer layers\n",
        "        self.use_multi_scale = Config.use_multi_scale\n",
        "\n",
        "        # Add projection layers for each scale\n",
        "        self.projections = nn.ModuleList([\n",
        "            nn.Linear(self.model.config.hidden_size, 256)\n",
        "            for _ in range(4)  # Use last 4 layers\n",
        "        ])\n",
        "\n",
        "        # Fusion layer for multi-scale features\n",
        "        self.fusion = nn.Conv2d(256*4, 256, kernel_size=1) if self.use_multi_scale else None\n",
        "\n",
        "        # For synthetic multi-scale if real ones not available\n",
        "        if Config.synthetic_multi_scale:\n",
        "            # Projection for synthetic multi-scale features\n",
        "            self.syn_projection = nn.Linear(self.model.config.hidden_size, 256)\n",
        "            # Fusion for synthetic features (3 scales: original, 2x2 pooled, 4x4 pooled)\n",
        "            self.syn_fusion = nn.Conv2d(256*3, 256, kernel_size=1)\n",
        "\n",
        "        # Segmentation head with convolutional layers\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 1, kernel_size=1)\n",
        "        )\n",
        "\n",
        "        # Store activation maps for GradCAM\n",
        "        self.activation = {}\n",
        "        self.gradients = {}\n",
        "        self._register_hooks()\n",
        "\n",
        "        # Flag to indicate if we had to fall back to single-scale\n",
        "        self.using_fallback = False\n",
        "        self.using_synthetic = False\n",
        "\n",
        "    def _register_hooks(self):\n",
        "        # Register hooks for GradCAM\n",
        "        def get_activation(name):\n",
        "            def hook(module, input, output):\n",
        "                self.activation[name] = output\n",
        "            return hook\n",
        "\n",
        "        def get_gradient(name):\n",
        "            def hook(module, grad_in, grad_out):\n",
        "                self.gradients[name] = grad_out[0]\n",
        "            return hook\n",
        "\n",
        "        # Register hooks for the last convolutional layer\n",
        "        self.head[0].register_forward_hook(get_activation('conv_feature'))\n",
        "        self.head[0].register_backward_hook(get_gradient('conv_feature'))\n",
        "\n",
        "    def create_synthetic_multi_scale(self, embeddings):\n",
        "        \"\"\"Create synthetic multi-scale features from a single embedding layer\"\"\"\n",
        "        # Original scale\n",
        "        b, n, c = embeddings.shape\n",
        "\n",
        "        # Project embeddings\n",
        "        projected = self.syn_projection(embeddings)\n",
        "\n",
        "        # Reshape to spatial dimensions (excluding CLS token)\n",
        "        spatial = projected.reshape(b, Config.patch_grid, Config.patch_grid, -1).permute(0, 3, 1, 2)\n",
        "\n",
        "        # Create multi-scale features\n",
        "        features = [spatial]  # Original scale\n",
        "\n",
        "        # Scale 2: Pooled 2x2\n",
        "        pooled2 = F.avg_pool2d(spatial, kernel_size=2, stride=2)\n",
        "        upsampled2 = F.interpolate(pooled2, size=(Config.patch_grid, Config.patch_grid), mode='bilinear', align_corners=False)\n",
        "        features.append(upsampled2)\n",
        "\n",
        "        # Scale 3: Pooled 4x4\n",
        "        pooled4 = F.avg_pool2d(spatial, kernel_size=4, stride=4)\n",
        "        upsampled4 = F.interpolate(pooled4, size=(Config.patch_grid, Config.patch_grid), mode='bilinear', align_corners=False)\n",
        "        features.append(upsampled4)\n",
        "\n",
        "        # Concatenate along channel dimension\n",
        "        multi_scale = torch.cat(features, dim=1)\n",
        "\n",
        "        # Fuse multi-scale features\n",
        "        fused = self.syn_fusion(multi_scale)\n",
        "\n",
        "        return fused\n",
        "\n",
        "    def forward(self, pixel_inputs):\n",
        "        # Get outputs with attention (for visualization)\n",
        "        outputs = self.model(\n",
        "            pixel_values=pixel_inputs,\n",
        "            output_hidden_states=self.use_multi_scale,\n",
        "            output_attentions=True\n",
        "        )\n",
        "\n",
        "        # Store attention maps for visualization if available\n",
        "        self.last_attentions = outputs.image_attentions if hasattr(outputs, 'image_attentions') else None\n",
        "\n",
        "        # Check if multi-scale is actually available in this model version\n",
        "        multi_scale_available = hasattr(outputs, 'image_hidden_states') and outputs.image_hidden_states is not None\n",
        "\n",
        "        # Log the first time we have to fall back\n",
        "        if self.use_multi_scale and not multi_scale_available and not self.using_fallback:\n",
        "            print(\"WARNING: Multi-scale features not available in this FLAVA model version.\")\n",
        "            print(\"Available outputs:\", list(outputs.keys()))\n",
        "            print(\"Falling back to single-scale features.\")\n",
        "            self.using_fallback = True\n",
        "\n",
        "        if self.use_multi_scale and multi_scale_available:\n",
        "            # Use last 4 layers\n",
        "            hidden_states = outputs.image_hidden_states[-4:]\n",
        "            multi_scale_features = []\n",
        "\n",
        "            for i, hidden_state in enumerate(hidden_states):\n",
        "                # Skip CLS token\n",
        "                patches = hidden_state[:, 1:, :]\n",
        "                b, n, c = patches.shape\n",
        "                # Project and reshape to spatial dimensions\n",
        "                projected = self.projections[i](patches)\n",
        "                spatial = projected.reshape(b, Config.patch_grid, Config.patch_grid, -1).permute(0, 3, 1, 2)\n",
        "                multi_scale_features.append(spatial)\n",
        "\n",
        "            # Concatenate features along channel dimension\n",
        "            fused_features = torch.cat(multi_scale_features, dim=1)\n",
        "            # Fuse multi-scale features\n",
        "            fused_features = self.fusion(fused_features)\n",
        "            # Apply segmentation head\n",
        "            seg_logits = self.head(fused_features)\n",
        "\n",
        "        elif Config.synthetic_multi_scale and not self.using_synthetic:\n",
        "            # Use synthetic multi-scale features\n",
        "            print(\"Using synthetic multi-scale features\")\n",
        "            self.using_synthetic = True\n",
        "\n",
        "            # Get image embeddings\n",
        "            patches = outputs.image_embeddings[:, 1:, :]  # Skip CLS token\n",
        "\n",
        "            # Create synthetic multi-scale features\n",
        "            fused_features = self.create_synthetic_multi_scale(patches)\n",
        "\n",
        "            # Apply segmentation head\n",
        "            seg_logits = self.head(fused_features)\n",
        "\n",
        "        else:\n",
        "            # Single-scale approach using image embeddings\n",
        "            patches = outputs.image_embeddings[:, 1:, :]  # Skip CLS token\n",
        "            b, n, c = patches.shape\n",
        "            projected = self.projections[0](patches)\n",
        "            spatial = projected.reshape(b, Config.patch_grid, Config.patch_grid, -1).permute(0, 3, 1, 2)\n",
        "            seg_logits = self.head(spatial)\n",
        "\n",
        "        return seg_logits\n",
        "\n",
        "    def get_gradcam(self, target_layer='conv_feature'):\n",
        "        \"\"\"Generate GradCAM heatmap for interpretability\"\"\"\n",
        "        if target_layer not in self.activation or target_layer not in self.gradients:\n",
        "            print(f\"Warning: {target_layer} activations or gradients not found\")\n",
        "            return None\n",
        "\n",
        "        # Get activations and gradients for the target layer\n",
        "        activations = self.activation[target_layer]\n",
        "        gradients = self.gradients[target_layer]\n",
        "\n",
        "        # Global average pooling of gradients\n",
        "        weights = torch.mean(gradients, dim=(2, 3), keepdim=True)\n",
        "\n",
        "        # Weighted sum of activation maps\n",
        "        cam = torch.sum(weights * activations, dim=1, keepdim=True)\n",
        "\n",
        "        # Apply ReLU to focus on features that have a positive influence\n",
        "        cam = F.relu(cam)\n",
        "\n",
        "        # Normalize\n",
        "        if torch.max(cam) > 0:\n",
        "            cam = cam / torch.max(cam)\n",
        "\n",
        "        return cam\n",
        "\n",
        "# ==== EVALUATION FUNCTIONS ====\n",
        "def calculate_metrics(preds, masks):\n",
        "    \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n",
        "    # IoU\n",
        "    intersection = (preds * masks).sum((1,2))\n",
        "    union = ((preds + masks) >= 1).float().sum((1,2))\n",
        "    batch_iou = (intersection / (union + 1e-6))\n",
        "\n",
        "    # Dice\n",
        "    dice = (2 * intersection) / (preds.sum((1,2)) + masks.sum((1,2)) + 1e-6)\n",
        "\n",
        "    # Precision, Recall, F1\n",
        "    tp = (preds * masks).sum((1,2))\n",
        "    fp = (preds * (1-masks)).sum((1,2))\n",
        "    fn = ((1-preds) * masks).sum((1,2))\n",
        "    tn = ((1-preds) * (1-masks)).sum((1,2))\n",
        "\n",
        "    precision = tp / (tp + fp + 1e-6)\n",
        "    recall = tp / (tp + fn + 1e-6)\n",
        "    f1 = 2 * precision * recall / (precision + recall + 1e-6)\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn + 1e-6)\n",
        "    specificity = tn / (tn + fp + 1e-6)\n",
        "\n",
        "    metrics = {\n",
        "        'iou': batch_iou.mean().item(),\n",
        "        'dice': dice.mean().item(),\n",
        "        'precision': precision.mean().item(),\n",
        "        'recall': recall.mean().item(),\n",
        "        'f1': f1.mean().item(),\n",
        "        'accuracy': accuracy.mean().item(),\n",
        "        'specificity': specificity.mean().item()\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def evaluate_model(model, dataloader, processor):\n",
        "    \"\"\"Evaluate model on a dataset\"\"\"\n",
        "    model.eval()\n",
        "    all_metrics = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, masks, _ in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            imgs, masks = imgs.to(Config.device), masks.to(Config.device)\n",
        "            pixel_inputs = processor(images=imgs, return_tensors=\"pt\")[\"pixel_values\"].to(Config.device)\n",
        "\n",
        "            logits = model(pixel_inputs).squeeze(1)\n",
        "            preds = (torch.sigmoid(logits) > 0.5).float()\n",
        "\n",
        "            batch_metrics = calculate_metrics(preds, masks)\n",
        "            all_metrics.append(batch_metrics)\n",
        "\n",
        "    # Calculate mean metrics across all batches\n",
        "    results = {}\n",
        "    for metric in all_metrics[0].keys():\n",
        "        results[metric] = np.mean([m[metric] for m in all_metrics])\n",
        "\n",
        "    return results\n",
        "\n",
        "# ==== VISUALIZATION FUNCTIONS ====\n",
        "def visualize_attention(model, image_tensor, processor, save_path, img_path=None):\n",
        "    \"\"\"\n",
        "    Visualize and analyze attention maps from FLAVA for interpretability analysis\n",
        "\n",
        "    Args:\n",
        "        model: The FLAVA segmentation model\n",
        "        image_tensor: Input image tensor\n",
        "        processor: FLAVA processor\n",
        "        save_path: Path to save visualizations\n",
        "        img_path: Optional original image path for naming\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with attention statistics\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Ensure batch dimension\n",
        "        if len(image_tensor.shape) == 3:\n",
        "            image_tensor = image_tensor.unsqueeze(0)\n",
        "\n",
        "        # Get pixel values using the processor\n",
        "        pixel_values = processor(images=image_tensor, return_tensors=\"pt\")[\"pixel_values\"].to(Config.device)\n",
        "\n",
        "        # Forward pass with attention outputs\n",
        "        outputs = model.model(pixel_values=pixel_values, output_attentions=True)\n",
        "\n",
        "        # Check if attention maps are available\n",
        "        if not hasattr(outputs, 'image_attentions') or outputs.image_attentions is None:\n",
        "            print(\"WARNING: No attention maps available for visualization.\")\n",
        "            attention_stats = {\n",
        "                'mean_entropy': 0.0,\n",
        "                'min_entropy': 0.0,\n",
        "                'max_entropy': 0.0,\n",
        "                'num_patches_50pct': 0,\n",
        "                'num_patches_90pct': 0,\n",
        "                'attention_concentration': 0.0\n",
        "            }\n",
        "            return attention_stats\n",
        "\n",
        "        # Get segmentation prediction\n",
        "        logits = model(pixel_values).squeeze(1)\n",
        "        pred = (torch.sigmoid(logits) > 0.5).float()\n",
        "\n",
        "        # Extract attention patterns\n",
        "        attentions = outputs.image_attentions  # List of tensors [batch, heads, seq_len, seq_len]\n",
        "\n",
        "        # Get number of attention layers and heads\n",
        "        num_layers = len(attentions)\n",
        "        num_heads = attentions[0].shape[1]\n",
        "\n",
        "        # Create base filename\n",
        "        base_filename = os.path.basename(img_path) if img_path else \"attention_analysis\"\n",
        "\n",
        "        # Create figure for attention visualization across layers\n",
        "        plt.figure(figsize=(15, num_layers * 3))\n",
        "\n",
        "        # 1. Analyze attention from CLS token to patches across layers\n",
        "        cls_attentions = []\n",
        "        for layer_idx, layer_attn in enumerate(attentions):\n",
        "            # Average across heads for CLS token attention\n",
        "            cls_attn = layer_attn[0, :, 0, 1:].mean(0)  # [seq_len-1]\n",
        "            cls_attn_map = cls_attn.reshape(Config.patch_grid, Config.patch_grid).cpu().numpy()\n",
        "            cls_attentions.append(cls_attn_map)\n",
        "\n",
        "            plt.subplot(num_layers, 3, layer_idx*3 + 1)\n",
        "            plt.imshow(cls_attn_map, cmap='viridis')\n",
        "            plt.title(f\"Layer {layer_idx+1}: CLS Attention\")\n",
        "            plt.colorbar(fraction=0.046, pad=0.04)\n",
        "            plt.axis('off')\n",
        "\n",
        "        # 2. Create attention heatmap overlaid on the original image\n",
        "        image_np = image_tensor[0].cpu().permute(1, 2, 0).numpy()\n",
        "\n",
        "        # Use last layer's attention for overlay\n",
        "        last_layer_cls_attn = cls_attentions[-1]\n",
        "\n",
        "        # Resize attention map to match image size\n",
        "        h, w = image_np.shape[:2]\n",
        "        attn_resized = np.array(Image.fromarray(last_layer_cls_attn).resize((w, h)))\n",
        "\n",
        "        # Normalize attention for visualization\n",
        "        attn_normalized = (attn_resized - attn_resized.min()) / (attn_resized.max() - attn_resized.min() + 1e-8)\n",
        "\n",
        "        # Create attention heatmap overlay\n",
        "        plt.subplot(num_layers, 3, 2)\n",
        "        plt.imshow(image_np)\n",
        "        plt.imshow(attn_normalized, cmap='hot', alpha=0.5)\n",
        "        plt.title(\"Attention Overlay\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # 3. Compare attention with segmentation prediction\n",
        "        plt.subplot(num_layers, 3, 3)\n",
        "        plt.imshow(pred[0].cpu().numpy(), cmap='gray')\n",
        "        plt.title(\"Segmentation Prediction\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Save the combined visualization\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(save_path, f\"{base_filename}_attention_layers.png\"), dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "        # 4. Analyze attention heads in the last layer\n",
        "        plt.figure(figsize=(15, 10))\n",
        "        n_cols = 4\n",
        "        n_rows = (num_heads + n_cols - 1) // n_cols\n",
        "\n",
        "        for head_idx in range(num_heads):\n",
        "            head_attn = attentions[-1][0, head_idx, 0, 1:].reshape(Config.patch_grid, Config.patch_grid).cpu().numpy()\n",
        "\n",
        "            plt.subplot(n_rows, n_cols, head_idx + 1)\n",
        "            plt.imshow(head_attn, cmap='viridis')\n",
        "            plt.title(f\"Head {head_idx + 1}\")\n",
        "            plt.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(save_path, f\"{base_filename}_attention_heads.png\"), dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "        # 5. Calculate attention statistics\n",
        "        attention_stats = {}\n",
        "\n",
        "        # Mean attention entropy (measures attention distribution)\n",
        "        entropy_values = []\n",
        "        for layer_idx in range(num_layers):\n",
        "            for head_idx in range(num_heads):\n",
        "                # Get attention distribution from CLS token\n",
        "                attn_dist = attentions[layer_idx][0, head_idx, 0, 1:].cpu().numpy()\n",
        "                # Normalize to get probability distribution\n",
        "                attn_dist = attn_dist / (attn_dist.sum() + 1e-10)\n",
        "                # Calculate entropy: -sum(p * log(p))\n",
        "                entropy = -np.sum(attn_dist * np.log(attn_dist + 1e-10))\n",
        "                entropy_values.append(entropy)\n",
        "\n",
        "        attention_stats['mean_entropy'] = np.mean(entropy_values)\n",
        "        attention_stats['min_entropy'] = np.min(entropy_values)\n",
        "        attention_stats['max_entropy'] = np.max(entropy_values)\n",
        "\n",
        "        # Attention concentration (how much attention focuses on top k% of patches)\n",
        "        last_layer_attn = attentions[-1][0, :, 0, 1:].mean(0).cpu().numpy()  # Average across heads\n",
        "        sorted_attn = np.sort(last_layer_attn)[::-1]  # Sort in descending order\n",
        "        cumsum_attn = np.cumsum(sorted_attn) / np.sum(sorted_attn)\n",
        "\n",
        "        # Find how many patches capture 50% and 90% of attention\n",
        "        attention_stats['num_patches_50pct'] = np.argmax(cumsum_attn >= 0.5) + 1\n",
        "        attention_stats['num_patches_90pct'] = np.argmax(cumsum_attn >= 0.9) + 1\n",
        "        attention_stats['attention_concentration'] = attention_stats['num_patches_50pct'] / len(last_layer_attn)\n",
        "\n",
        "        # 6. Plot attention concentration curve\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(range(1, len(cumsum_attn) + 1), cumsum_attn, 'b-')\n",
        "        plt.axhline(y=0.5, color='r', linestyle='--', label='50% attention')\n",
        "        plt.axhline(y=0.9, color='g', linestyle='--', label='90% attention')\n",
        "        plt.axvline(x=attention_stats['num_patches_50pct'], color='r', linestyle=':')\n",
        "        plt.axvline(x=attention_stats['num_patches_90pct'], color='g', linestyle=':')\n",
        "        plt.xlabel('Number of Patches (sorted by attention)')\n",
        "        plt.ylabel('Cumulative Attention')\n",
        "        plt.title('Attention Concentration Analysis')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.savefig(os.path.join(save_path, f\"{base_filename}_attention_concentration.png\"), dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "        # Save statistics to JSON\n",
        "        with open(os.path.join(save_path, f\"{base_filename}_attention_stats.json\"), 'w') as f:\n",
        "            json.dump(attention_stats, f, indent=2)\n",
        "\n",
        "        return attention_stats\n",
        "\n",
        "def visualize_batch(model, imgs, masks, paths, epoch, step, save_dir):\n",
        "    \"\"\"Create debug visualizations for a batch\"\"\"\n",
        "    # Forward pass to get predictions - using no_grad to avoid gradient tracking\n",
        "    with torch.no_grad():\n",
        "        logits = model(imgs)\n",
        "        preds = torch.sigmoid(logits)\n",
        "        preds_binary = (preds > 0.5).float()\n",
        "\n",
        "        # Get attention maps if available\n",
        "        attention_maps = None\n",
        "        if hasattr(model, 'last_attentions') and model.last_attentions is not None:\n",
        "            try:\n",
        "                # Last layer, mean over heads, first token (CLS)\n",
        "                attention_maps = model.last_attentions[-1].mean(1)[:, 0, 1:].reshape(-1, Config.patch_grid, Config.patch_grid)\n",
        "            except (IndexError, AttributeError) as e:\n",
        "                print(f\"Warning: Could not extract attention maps: {e}\")\n",
        "\n",
        "    # Create directory if it doesn't exist\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    for i in range(min(2, imgs.shape[0])):  # Visualize up to 2 samples\n",
        "        try:\n",
        "            name = os.path.basename(paths[i])\n",
        "\n",
        "            # Make sure everything is detached from computation graph\n",
        "            mask_np = masks[i].detach().cpu().numpy()\n",
        "            pred_np = preds[i].detach().cpu().squeeze().numpy()\n",
        "            pred_binary_np = preds_binary[i].detach().cpu().squeeze().numpy()\n",
        "            img_np = imgs[i].detach().cpu().permute(1, 2, 0).numpy()\n",
        "\n",
        "            # Calculate metrics\n",
        "            with torch.no_grad():\n",
        "                batch_metrics = calculate_metrics(preds_binary[i:i+1], masks[i:i+1])\n",
        "\n",
        "            # Create figure with appropriate number of subplots\n",
        "            n_plots = 5 if attention_maps is not None else 4\n",
        "            plt.figure(figsize=(n_plots*3, 3))\n",
        "\n",
        "            # Input image\n",
        "            plt.subplot(1, n_plots, 1)\n",
        "            plt.imshow(img_np)\n",
        "            plt.title(\"Input Image\")\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Ground truth mask\n",
        "            plt.subplot(1, n_plots, 2)\n",
        "            plt.imshow(mask_np, cmap='gray')\n",
        "            plt.title(\"Ground Truth\")\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Prediction probability\n",
        "            plt.subplot(1, n_plots, 3)\n",
        "            plt.imshow(pred_np, cmap='hot')\n",
        "            plt.title(\"Prediction (Prob)\")\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Binary prediction\n",
        "            plt.subplot(1, n_plots, 4)\n",
        "            plt.imshow(pred_binary_np, cmap='gray')\n",
        "            plt.title(f\"Binary Pred (IoU: {batch_metrics['iou']:.2f})\")\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Attention map (if available)\n",
        "            if attention_maps is not None:\n",
        "                attn_np = attention_maps[i].detach().cpu().numpy()\n",
        "                plt.subplot(1, n_plots, 5)\n",
        "                plt.imshow(attn_np, cmap='viridis')\n",
        "                plt.title(\"Attention Map\")\n",
        "                plt.axis('off')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(save_dir, f\"epoch{epoch+1}_step{step}_{name}\"))\n",
        "            plt.close()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Error in visualization for sample {i}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "def plot_training_curves(metrics_history, save_path):\n",
        "    \"\"\"Plot training and validation metrics over epochs\"\"\"\n",
        "    epochs = range(1, len(metrics_history['train_loss']) + 1)\n",
        "\n",
        "    # Create subplots for each metric\n",
        "    metric_groups = [\n",
        "        ['loss'],\n",
        "        ['iou', 'dice'],\n",
        "        ['precision', 'recall', 'f1'],\n",
        "        ['accuracy', 'specificity']\n",
        "    ]\n",
        "\n",
        "    fig, axes = plt.subplots(len(metric_groups), 1, figsize=(10, 4*len(metric_groups)))\n",
        "\n",
        "    for i, metrics in enumerate(metric_groups):\n",
        "        ax = axes[i]\n",
        "\n",
        "        for metric in metrics:\n",
        "            if f'train_{metric}' in metrics_history:\n",
        "                ax.plot(epochs, metrics_history[f'train_{metric}'], 'b-', label=f'Train {metric}')\n",
        "            if f'val_{metric}' in metrics_history:\n",
        "                ax.plot(epochs, metrics_history[f'val_{metric}'], 'r-', label=f'Val {metric}')\n",
        "\n",
        "        ax.set_xlabel('Epoch')\n",
        "        ax.set_ylabel(' / '.join(metrics).capitalize())\n",
        "        ax.set_title(f\"{' / '.join(metrics).capitalize()} over Epochs\")\n",
        "        ax.legend()\n",
        "        ax.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n",
        "\n",
        "# ==== TRAINING ====\n",
        "def train():\n",
        "    # متغیرهای مربوط به بهترین مدل\n",
        "    best_val_iou = 0\n",
        "    best_epoch = 0\n",
        "\n",
        "    # Load dataset\n",
        "    full_dataset = MaskedDataset(Config.data_path)\n",
        "\n",
        "    # Split into train and validation sets\n",
        "    train_indices, val_indices = train_test_split(\n",
        "        range(len(full_dataset)),\n",
        "        test_size=Config.test_split,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    train_dataset = torch.utils.data.Subset(full_dataset, train_indices)\n",
        "    val_dataset = torch.utils.data.Subset(full_dataset, val_indices)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=Config.batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=Config.batch_size, shuffle=False)\n",
        "\n",
        "    print(f\"Training on {len(train_dataset)} samples, validating on {len(val_dataset)} samples\")\n",
        "\n",
        "    # Initialize model and optimizer\n",
        "    processor = FlavaProcessor.from_pretrained(Config.base_model_path)\n",
        "    processor.image_processor.do_rescale = False\n",
        "\n",
        "    model = FLAVASegmenter(Config.base_model_path).to(Config.device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=Config.lr)\n",
        "\n",
        "    # Initialize loss function\n",
        "    if Config.use_advanced_loss:\n",
        "        loss_fn = FocalDiceLoss()\n",
        "        print(\"Using FocalDiceLoss\")\n",
        "    else:\n",
        "        loss_fn = nn.BCEWithLogitsLoss()\n",
        "        print(\"Using BCEWithLogitsLoss\")\n",
        "\n",
        "    # Initialize metrics history\n",
        "    metrics_history = {\n",
        "        'train_loss': [], 'val_loss': [],\n",
        "        'train_iou': [], 'val_iou': [],\n",
        "        'train_dice': [], 'val_dice': [],\n",
        "        'train_precision': [], 'val_precision': [],\n",
        "        'train_recall': [], 'val_recall': [],\n",
        "        'train_f1': [], 'val_f1': [],\n",
        "        'train_accuracy': [], 'val_accuracy': [],\n",
        "        'train_specificity': [], 'val_specificity': []\n",
        "    }\n",
        "\n",
        "    # Create directory for attention analysis\n",
        "    attention_dir = os.path.join(Config.save_path, \"attention_analysis\")\n",
        "    os.makedirs(attention_dir, exist_ok=True)\n",
        "\n",
        "    # Check if model has attention capabilities\n",
        "    # Try a test forward pass to see if we get attention maps\n",
        "    sample_img, _, _ = full_dataset[0]\n",
        "    sample_img = sample_img.unsqueeze(0).to(Config.device)\n",
        "    pixel_inputs = processor(images=sample_img, return_tensors=\"pt\")[\"pixel_values\"].to(Config.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.model(pixel_values=pixel_inputs, output_attentions=True)\n",
        "        has_attention = hasattr(outputs, 'image_attentions') and outputs.image_attentions is not None\n",
        "\n",
        "    if not has_attention:\n",
        "        print(\"WARNING: This FLAVA model doesn't provide attention maps. Attention visualization will be disabled.\")\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(Config.num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        train_metrics_list = []\n",
        "\n",
        "        for step, (imgs, masks, paths) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\")):\n",
        "            imgs, masks = imgs.to(Config.device), masks.to(Config.device)\n",
        "            pixel_inputs = processor(images=imgs, return_tensors=\"pt\")[\"pixel_values\"].to(Config.device)\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(pixel_inputs).squeeze(1)\n",
        "            loss = loss_fn(logits, masks)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Calculate metrics\n",
        "            preds = (torch.sigmoid(logits) > 0.5).float()\n",
        "            batch_metrics = calculate_metrics(preds, masks)\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "            train_metrics_list.append(batch_metrics)\n",
        "\n",
        "            # Debug visualization\n",
        "            if step % 20 == 0:\n",
        "                try:\n",
        "                    visualize_batch(model, imgs, masks, paths, epoch, step, Config.debug_dir)\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Error in batch visualization: {e}\")\n",
        "\n",
        "                # Run detailed attention analysis on one sample every 50 steps\n",
        "                if has_attention and step % 50 == 0 and step > 0:\n",
        "                    try:\n",
        "                        attention_stats = visualize_attention(\n",
        "                            model,\n",
        "                            imgs[0],\n",
        "                            processor,\n",
        "                            attention_dir,\n",
        "                            img_path=paths[0]\n",
        "                        )\n",
        "                        print(f\"  Attention stats - Concentration: {attention_stats['attention_concentration']:.4f}, Entropy: {attention_stats['mean_entropy']:.4f}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"Warning: Error in attention visualization: {e}\")\n",
        "\n",
        "        # Calculate average training metrics\n",
        "        avg_train_loss = np.mean(train_losses)\n",
        "        avg_train_metrics = {}\n",
        "        for metric in train_metrics_list[0].keys():\n",
        "            avg_train_metrics[metric] = np.mean([m[metric] for m in train_metrics_list])\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        val_metrics_list = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for step, (imgs, masks, paths) in enumerate(tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\")):\n",
        "                imgs, masks = imgs.to(Config.device), masks.to(Config.device)\n",
        "                pixel_inputs = processor(images=imgs, return_tensors=\"pt\")[\"pixel_values\"].to(Config.device)\n",
        "\n",
        "                # Forward pass\n",
        "                logits = model(pixel_inputs).squeeze(1)\n",
        "                loss = loss_fn(logits, masks)\n",
        "\n",
        "                # Calculate metrics\n",
        "                preds = (torch.sigmoid(logits) > 0.5).float()\n",
        "                batch_metrics = calculate_metrics(preds, masks)\n",
        "\n",
        "                val_losses.append(loss.item())\n",
        "                val_metrics_list.append(batch_metrics)\n",
        "\n",
        "                # Debug visualization for validation\n",
        "                if step % 10 == 0:\n",
        "                    try:\n",
        "                        visualize_batch(model, imgs, masks, paths, epoch, f\"val_{step}\", Config.debug_dir)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Warning: Error in validation visualization: {e}\")\n",
        "\n",
        "                    # Run detailed attention analysis on validation samples\n",
        "                    if has_attention and step == 0:\n",
        "                        try:\n",
        "                            attention_stats = visualize_attention(\n",
        "                                model,\n",
        "                                imgs[0],\n",
        "                                processor,\n",
        "                                attention_dir,\n",
        "                                img_path=f\"val_epoch{epoch+1}_{os.path.basename(paths[0])}\"\n",
        "                            )\n",
        "                        except Exception as e:\n",
        "                            print(f\"Warning: Error in validation attention visualization: {e}\")\n",
        "\n",
        "        # Calculate average validation metrics\n",
        "        avg_val_loss = np.mean(val_losses)\n",
        "        avg_val_metrics = {}\n",
        "        for metric in val_metrics_list[0].keys():\n",
        "            avg_val_metrics[metric] = np.mean([m[metric] for m in val_metrics_list])\n",
        "\n",
        "        # Update metrics history\n",
        "        metrics_history['train_loss'].append(avg_train_loss)\n",
        "        metrics_history['val_loss'].append(avg_val_loss)\n",
        "\n",
        "        for metric in avg_train_metrics:\n",
        "            metrics_history[f'train_{metric}'].append(avg_train_metrics[metric])\n",
        "            metrics_history[f'val_{metric}'].append(avg_val_metrics[metric])\n",
        "\n",
        "        # ذخیره بهترین مدل بر اساس IoU\n",
        "        if epoch == 0 or avg_val_metrics['iou'] > best_val_iou:\n",
        "            best_val_iou = avg_val_metrics['iou']\n",
        "            best_epoch = epoch + 1\n",
        "            torch.save(model.state_dict(), os.path.join(Config.save_path, \"flava_seg_head_enhanced_best.pth\"))\n",
        "            print(f\"  New best model saved! IoU: {best_val_iou:.4f}\")\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f\"\\n[Epoch {epoch+1}/{Config.num_epochs}] Summary:\")\n",
        "        print(f\"  Train - Loss: {avg_train_loss:.4f}, IoU: {avg_train_metrics['iou']:.4f}, Dice: {avg_train_metrics['dice']:.4f}\")\n",
        "        print(f\"  Validation - Loss: {avg_val_loss:.4f}, IoU: {avg_val_metrics['iou']:.4f}, Dice: {avg_val_metrics['dice']:.4f}\")\n",
        "\n",
        "        # Plot training curves\n",
        "        plot_training_curves(metrics_history, os.path.join(Config.plots_dir, f\"training_curves_epoch{epoch+1}.png\"))\n",
        "\n",
        "        # Save metrics as CSV\n",
        "        metrics_df = pd.DataFrame(metrics_history)\n",
        "        metrics_df.to_csv(os.path.join(Config.metrics_dir, \"training_metrics.csv\"))\n",
        "\n",
        "        # Save model checkpoint\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'train_loss': avg_train_loss,\n",
        "            'val_loss': avg_val_loss,\n",
        "            'metrics': metrics_history\n",
        "        }, os.path.join(Config.save_path, f\"model_checkpoint_epoch{epoch+1}.pth\"))\n",
        "\n",
        "    # Save final model\n",
        "    torch.save(model.state_dict(), os.path.join(Config.save_path, \"flava_seg_head_enhanced_final.pth\"))\n",
        "\n",
        "    # Final plots and visualizations\n",
        "    plot_training_curves(metrics_history, os.path.join(Config.plots_dir, \"final_training_curves.png\"))\n",
        "\n",
        "    # Generate final attention analysis for paper-ready visualizations\n",
        "    if has_attention:\n",
        "        print(\"\\nGenerating final attention analysis visualizations...\")\n",
        "        # Select a few representative samples from validation set\n",
        "        num_samples = min(5, len(val_dataset))\n",
        "        for i in range(num_samples):\n",
        "            try:\n",
        "                img, mask, img_path = val_dataset[i]\n",
        "                attention_stats = visualize_attention(\n",
        "                    model,\n",
        "                    img.to(Config.device),\n",
        "                    processor,\n",
        "                    attention_dir,\n",
        "                    img_path=f\"final_sample_{i+1}_{os.path.basename(img_path)}\"\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Error in final attention visualization for sample {i}: {e}\")\n",
        "\n",
        "    print(f\"\\n✅ Model training completed! Results saved to: {Config.save_path}\")\n",
        "    print(f\"  Best validation IoU: {best_val_iou:.4f} (Epoch {best_epoch})\")\n",
        "    print(f\"  Final validation IoU: {metrics_history['val_iou'][-1]:.4f}\")\n",
        "    print(f\"  Best model saved to: {os.path.join(Config.save_path, 'flava_seg_head_enhanced_best.pth')}\")\n",
        "\n",
        "    if has_attention:\n",
        "        print(f\"  Attention analysis and visualizations saved to: {attention_dir}\")\n",
        "\n",
        "    return model, metrics_history\n",
        "\n",
        "def visualize_gradcam(model, image_tensor, mask_tensor, processor, save_path=None):\n",
        "    \"\"\"Visualize GradCAM to understand which parts of the image influence predictions\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Ensure batch dimension\n",
        "    if len(image_tensor.shape) == 3:\n",
        "        image_tensor = image_tensor.unsqueeze(0)\n",
        "    if len(mask_tensor.shape) == 2:\n",
        "        mask_tensor = mask_tensor.unsqueeze(0)\n",
        "\n",
        "    # Clear gradients\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    pixel_values = processor(images=image_tensor, return_tensors=\"pt\")[\"pixel_values\"].to(Config.device)\n",
        "    logits = model(pixel_values)\n",
        "\n",
        "    # Calculate prediction\n",
        "    pred = torch.sigmoid(logits)\n",
        "\n",
        "    # Backward pass to get gradients\n",
        "    pred.mean().backward()\n",
        "\n",
        "    # Get GradCAM\n",
        "    cam = model.get_gradcam()\n",
        "\n",
        "    if cam is None:\n",
        "        print(\"Warning: Could not generate GradCAM.\")\n",
        "        return None\n",
        "\n",
        "    # Resize CAM to match image size\n",
        "    cam_resized = F.interpolate(cam, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "\n",
        "    # Convert tensors to numpy for visualization\n",
        "    image_np = image_tensor[0].cpu().permute(1, 2, 0).numpy()\n",
        "    mask_np = mask_tensor[0].cpu().numpy()\n",
        "    pred_np = pred[0, 0].detach().cpu().numpy()\n",
        "    cam_np = cam_resized[0, 0].detach().cpu().numpy()\n",
        "\n",
        "    if save_path:\n",
        "        # Save visualization\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        plt.subplot(1, 4, 1)\n",
        "        plt.imshow(image_np)\n",
        "        plt.title(\"Original Image\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1, 4, 2)\n",
        "        plt.imshow(mask_np, cmap='gray')\n",
        "        plt.title(\"Ground Truth\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1, 4, 3)\n",
        "        plt.imshow(pred_np, cmap='gray')\n",
        "        plt.title(\"Prediction\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1, 4, 4)\n",
        "        plt.imshow(image_np)\n",
        "        plt.imshow(cam_np, cmap='jet', alpha=0.5)\n",
        "        plt.title(\"GradCAM Overlay\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "    return cam_np\n",
        "\n",
        "def analyze_feature_embeddings(model, dataset, processor, save_dir, num_samples=100):\n",
        "    \"\"\"Analyze embedding space to understand model behavior\"\"\"\n",
        "    # Limit the number of samples to analyze\n",
        "    num_samples = min(num_samples, len(dataset))\n",
        "    indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
        "\n",
        "    # Collect embeddings and metadata\n",
        "    embeddings = []\n",
        "    labels = []\n",
        "    filenames = []\n",
        "    metrics = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(indices, desc=\"Extracting embeddings\"):\n",
        "            img, mask, img_path = dataset[i]\n",
        "\n",
        "            # Get class from path\n",
        "            try:\n",
        "                class_name = img_path.split('/')[-2]  # Assuming folder name is class name\n",
        "            except:\n",
        "                class_name = \"unknown\"\n",
        "\n",
        "            # Process image\n",
        "            pixel_values = processor(images=img.unsqueeze(0), return_tensors=\"pt\")[\"pixel_values\"].to(Config.device)\n",
        "\n",
        "            # Get embeddings\n",
        "            outputs = model.model(pixel_values)\n",
        "            embedding = outputs.image_embeddings[:, 0, :].cpu().numpy()  # CLS token\n",
        "\n",
        "            # Get prediction and calculate metrics\n",
        "            logits = model(pixel_values).squeeze()\n",
        "            pred = (torch.sigmoid(logits) > 0.5).float()\n",
        "            metric = calculate_metrics(pred.unsqueeze(0), mask.unsqueeze(0).to(Config.device))\n",
        "\n",
        "            embeddings.append(embedding)\n",
        "            labels.append(class_name)\n",
        "            filenames.append(os.path.basename(img_path))\n",
        "            metrics.append(metric)\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    embeddings = np.vstack(embeddings)\n",
        "\n",
        "    # Run t-SNE to visualize embedding space\n",
        "    tsne = TSNE(n_components=2, random_state=42)\n",
        "    embeddings_2d = tsne.fit_transform(embeddings)\n",
        "\n",
        "    # Create DataFrame for easier plotting\n",
        "    df = pd.DataFrame({\n",
        "        'x': embeddings_2d[:, 0],\n",
        "        'y': embeddings_2d[:, 1],\n",
        "        'class': labels,\n",
        "        'filename': filenames,\n",
        "        'iou': [m['iou'] for m in metrics],\n",
        "        'dice': [m['dice'] for m in metrics]\n",
        "    })\n",
        "\n",
        "    # Save the data for future analysis\n",
        "    df.to_csv(os.path.join(save_dir, \"embedding_analysis.csv\"), index=False)\n",
        "\n",
        "    # Plot by class\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    unique_classes = df['class'].unique()\n",
        "\n",
        "    # Create a colormap\n",
        "    cmap = plt.cm.get_cmap('tab10', len(unique_classes))\n",
        "\n",
        "    for i, cls in enumerate(unique_classes):\n",
        "        subset = df[df['class'] == cls]\n",
        "        plt.scatter(subset['x'], subset['y'], c=[cmap(i)], label=cls, alpha=0.7)\n",
        "\n",
        "    plt.title('t-SNE Visualization of FLAVA Embeddings by Class')\n",
        "    plt.xlabel('t-SNE Dimension 1')\n",
        "    plt.ylabel('t-SNE Dimension 2')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(save_dir, \"tsne_by_class.png\"), dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    # Plot by IoU performance\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    scatter = plt.scatter(df['x'], df['y'], c=df['iou'], cmap='viridis', alpha=0.7)\n",
        "    plt.colorbar(scatter, label='IoU Score')\n",
        "    plt.title('t-SNE Visualization of FLAVA Embeddings by Segmentation Performance')\n",
        "    plt.xlabel('t-SNE Dimension 1')\n",
        "    plt.ylabel('t-SNE Dimension 2')\n",
        "    plt.savefig(os.path.join(save_dir, \"tsne_by_performance.png\"), dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    return df\n",
        "\n",
        "def statistical_analysis(model, dataset, processor, save_dir):\n",
        "    \"\"\"Perform advanced statistical analysis of model performance\"\"\"\n",
        "    metrics_by_class = {}\n",
        "    size_metrics = {}  # Correlate defect size with performance\n",
        "    location_metrics = {}  # Analyze performance by defect location\n",
        "\n",
        "    # Process each sample\n",
        "    for i in tqdm(range(len(dataset)), desc=\"Performing statistical analysis\"):\n",
        "        img, mask, img_path = dataset[i]\n",
        "\n",
        "        # Get class from path\n",
        "        try:\n",
        "            class_name = img_path.split('/')[-2]  # Assuming folder name is class name\n",
        "        except:\n",
        "            class_name = \"unknown\"\n",
        "\n",
        "        if class_name not in metrics_by_class:\n",
        "            metrics_by_class[class_name] = []\n",
        "\n",
        "        # Get prediction\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            pixel_values = processor(images=img.unsqueeze(0), return_tensors=\"pt\")[\"pixel_values\"].to(Config.device)\n",
        "            logits = model(pixel_values).squeeze()\n",
        "            pred = (torch.sigmoid(logits) > 0.5).float()\n",
        "\n",
        "            # Calculate metrics\n",
        "            metrics = calculate_metrics(pred.unsqueeze(0), mask.unsqueeze(0).to(Config.device))\n",
        "            metrics_by_class[class_name].append(metrics)\n",
        "\n",
        "            # Analyze defect size correlation\n",
        "            defect_size = mask.sum().item() / mask.numel()\n",
        "            size_bin = int(defect_size * 10)  # Create size bins (0-10%)\n",
        "            if size_bin not in size_metrics:\n",
        "                size_metrics[size_bin] = []\n",
        "            size_metrics[size_bin].append(metrics['iou'])\n",
        "\n",
        "            # Analyze defect location\n",
        "            # Calculate center of mass of the defect\n",
        "            if mask.sum() > 0:\n",
        "                indices = torch.nonzero(mask)\n",
        "                center_y = indices[:, 0].float().mean() / mask.shape[0]\n",
        "                center_x = indices[:, 1].float().mean() / mask.shape[1]\n",
        "\n",
        "                # Divide image into 3x3 grid\n",
        "                grid_y = min(2, int(center_y * 3))\n",
        "                grid_x = min(2, int(center_x * 3))\n",
        "                grid_pos = (grid_y * 3) + grid_x\n",
        "\n",
        "                if grid_pos not in location_metrics:\n",
        "                    location_metrics[grid_pos] = []\n",
        "                location_metrics[grid_pos].append(metrics['iou'])\n",
        "\n",
        "    # Generate class-wise performance report\n",
        "    class_performance = {cls: {metric: np.mean([m[metric] for m in metrics_list])\n",
        "                             for metric in ['iou', 'dice', 'precision', 'recall']}\n",
        "                       for cls, metrics_list in metrics_by_class.items()}\n",
        "\n",
        "    # Save as CSV\n",
        "    cls_df = pd.DataFrame(class_performance).T\n",
        "    cls_df.to_csv(os.path.join(save_dir, \"class_performance.csv\"))\n",
        "\n",
        "    # Generate class performance visualization\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    cls_df[['iou', 'dice', 'precision', 'recall']].plot(kind='bar')\n",
        "    plt.title('Performance Metrics by Class')\n",
        "    plt.ylabel('Score')\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_dir, \"class_performance.png\"), dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    # Generate size correlation plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sizes = []\n",
        "    ious = []\n",
        "    errors = []\n",
        "\n",
        "    for size_bin, iou_values in sorted(size_metrics.items()):\n",
        "        if len(iou_values) > 0:  # Only include bins with data\n",
        "            sizes.append(size_bin / 10)  # Convert bin back to percentage\n",
        "            ious.append(np.mean(iou_values))\n",
        "            errors.append(np.std(iou_values) / np.sqrt(len(iou_values)))  # Standard error\n",
        "\n",
        "    plt.errorbar(sizes, ious, yerr=errors, fmt='o-', capsize=5)\n",
        "    plt.xlabel('Defect Size (% of image)')\n",
        "    plt.ylabel('Mean IoU')\n",
        "    plt.title('Relationship Between Defect Size and Segmentation Performance')\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    plt.savefig(os.path.join(save_dir, \"size_performance_correlation.png\"), dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    # Generate location performance visualization (3x3 grid)\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    grid_values = np.zeros((3, 3))\n",
        "\n",
        "    for pos, values in location_metrics.items():\n",
        "        if len(values) > 0:  # Only include positions with data\n",
        "            row = pos // 3\n",
        "            col = pos % 3\n",
        "            grid_values[row, col] = np.mean(values)\n",
        "\n",
        "    ax = sns.heatmap(grid_values, annot=True, cmap='viridis', fmt='.3f', cbar_kws={'label': 'Mean IoU'})\n",
        "    ax.set_title('Segmentation Performance by Defect Location')\n",
        "    ax.set_xlabel('Horizontal Position')\n",
        "    ax.set_ylabel('Vertical Position')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_dir, \"location_performance.png\"), dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    # Perform statistical tests\n",
        "    # 1. Compare performance between different defect sizes\n",
        "    size_stats = {}\n",
        "    if len(size_metrics) > 1:\n",
        "        size_keys = sorted(size_metrics.keys())\n",
        "        for i in range(len(size_keys)):\n",
        "            for j in range(i+1, len(size_keys)):\n",
        "                if len(size_metrics[size_keys[i]]) > 5 and len(size_metrics[size_keys[j]]) > 5:\n",
        "                    t_stat, p_val = ttest_ind(size_metrics[size_keys[i]], size_metrics[size_keys[j]], equal_var=False)\n",
        "                    comparison = f\"{size_keys[i]/10:.1f}% vs {size_keys[j]/10:.1f}%\"\n",
        "                    size_stats[comparison] = {\n",
        "                        't_statistic': t_stat,\n",
        "                        'p_value': p_val,\n",
        "                        'significant': p_val < 0.05\n",
        "                    }\n",
        "\n",
        "    # 2. Compare performance between different classes\n",
        "    class_stats = {}\n",
        "    class_keys = list(metrics_by_class.keys())\n",
        "    for i in range(len(class_keys)):\n",
        "        for j in range(i+1, len(class_keys)):\n",
        "            if len(metrics_by_class[class_keys[i]]) > 5 and len(metrics_by_class[class_keys[j]]) > 5:\n",
        "                iou_i = [m['iou'] for m in metrics_by_class[class_keys[i]]]\n",
        "                iou_j = [m['iou'] for m in metrics_by_class[class_keys[j]]]\n",
        "                t_stat, p_val = ttest_ind(iou_i, iou_j, equal_var=False)\n",
        "                comparison = f\"{class_keys[i]} vs {class_keys[j]}\"\n",
        "                class_stats[comparison] = {\n",
        "                    't_statistic': t_stat,\n",
        "                    'p_value': p_val,\n",
        "                    'significant': p_val < 0.05\n",
        "                }\n",
        "\n",
        "    # Save statistical test results\n",
        "    with open(os.path.join(save_dir, \"statistical_tests.txt\"), 'w') as f:\n",
        "        f.write(\"Statistical Analysis of FLAVA Defect Segmentation\\n\")\n",
        "        f.write(\"===============================================\\n\\n\")\n",
        "\n",
        "        f.write(\"Size Comparison Tests (t-tests)\\n\")\n",
        "        f.write(\"-------------------------------\\n\")\n",
        "        for comparison, stats in size_stats.items():\n",
        "            f.write(f\"{comparison}: t={stats['t_statistic']:.4f}, p={stats['p_value']:.4f}\")\n",
        "            if stats['significant']:\n",
        "                f.write(\" (significant)\\n\")\n",
        "            else:\n",
        "                f.write(\" (not significant)\\n\")\n",
        "\n",
        "        f.write(\"\\nClass Comparison Tests (t-tests)\\n\")\n",
        "        f.write(\"-------------------------------\\n\")\n",
        "        for comparison, stats in class_stats.items():\n",
        "            f.write(f\"{comparison}: t={stats['t_statistic']:.4f}, p={stats['p_value']:.4f}\")\n",
        "            if stats['significant']:\n",
        "                f.write(\" (significant)\\n\")\n",
        "            else:\n",
        "                f.write(\" (not significant)\\n\")\n",
        "\n",
        "    return {\n",
        "        'class_performance': class_performance,\n",
        "        'size_metrics': size_metrics,\n",
        "        'location_metrics': location_metrics,\n",
        "        'size_stats': size_stats,\n",
        "        'class_stats': class_stats\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        model, metrics_history = train()\n",
        "\n",
        "        # Get validation dataset\n",
        "        full_dataset = MaskedDataset(Config.data_path)\n",
        "        _, val_indices = train_test_split(range(len(full_dataset)), test_size=Config.test_split, random_state=42)\n",
        "        val_dataset = torch.utils.data.Subset(full_dataset, val_indices)\n",
        "\n",
        "        # Load processor\n",
        "        processor = FlavaProcessor.from_pretrained(Config.base_model_path)\n",
        "        processor.image_processor.do_rescale = False\n",
        "\n",
        "        # Check if we can generate publication visualizations (depends on attention availability)\n",
        "        with torch.no_grad():\n",
        "            sample_img, _, _ = val_dataset[0]\n",
        "            sample_img = sample_img.unsqueeze(0).to(Config.device)\n",
        "            outputs = model.model(\n",
        "                pixel_values=processor(images=sample_img, return_tensors=\"pt\")[\"pixel_values\"].to(Config.device),\n",
        "                output_attentions=True\n",
        "            )\n",
        "            has_attention = hasattr(outputs, 'image_attentions') and outputs.image_attentions is not None\n",
        "\n",
        "        # Function to generate publication-quality visualizations of predictions only (no attention)\n",
        "        def generate_basic_visualizations(model, dataset, processor, save_dir, num_samples=5):\n",
        "            \"\"\"Generate basic prediction visualizations suitable for publication\"\"\"\n",
        "            pub_viz_dir = os.path.join(save_dir, \"publication_figures\")\n",
        "            os.makedirs(pub_viz_dir, exist_ok=True)\n",
        "\n",
        "            # Select representative samples\n",
        "            indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
        "\n",
        "            # Create a figure with multiple rows for samples and columns for visualizations\n",
        "            fig = plt.figure(figsize=(12, num_samples * 3))\n",
        "            gs = fig.add_gridspec(num_samples, 3, hspace=0.2, wspace=0.1)\n",
        "\n",
        "            for i, idx in enumerate(indices):\n",
        "                img, mask, img_path = dataset[idx]\n",
        "                img_name = os.path.basename(img_path)\n",
        "\n",
        "                # Process image and get predictions\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    pixel_values = processor(images=img.unsqueeze(0), return_tensors=\"pt\")[\"pixel_values\"].to(Config.device)\n",
        "                    logits = model(pixel_values).squeeze()\n",
        "                    pred = (torch.sigmoid(logits) > 0.5).float()\n",
        "\n",
        "                    # Calculate IoU and Dice\n",
        "                    iou = calculate_metrics(pred.unsqueeze(0), mask.unsqueeze(0).to(Config.device))['iou']\n",
        "                    dice = calculate_metrics(pred.unsqueeze(0), mask.unsqueeze(0).to(Config.device))['dice']\n",
        "\n",
        "                    # Error map (difference between prediction and ground truth)\n",
        "                    error_map = torch.abs(pred - mask.to(Config.device)).cpu().numpy()\n",
        "\n",
        "                # Original image\n",
        "                ax = fig.add_subplot(gs[i, 0])\n",
        "                ax.imshow(img.permute(1, 2, 0).cpu().numpy())\n",
        "                ax.set_title(\"Input Image\" if i == 0 else None)\n",
        "                ax.axis('off')\n",
        "\n",
        "                # Ground truth\n",
        "                ax = fig.add_subplot(gs[i, 1])\n",
        "                ax.imshow(mask.cpu().numpy(), cmap='gray')\n",
        "                ax.set_title(\"Ground Truth\" if i == 0 else None)\n",
        "                ax.axis('off')\n",
        "\n",
        "                # Model prediction with metrics\n",
        "                ax = fig.add_subplot(gs[i, 2])\n",
        "                pred_np = pred.cpu().numpy()\n",
        "                ax.imshow(pred_np, cmap='gray')\n",
        "                ax.set_title(f\"Prediction (IoU: {iou:.2f}, Dice: {dice:.2f})\" if i == 0 else f\"IoU: {iou:.2f}, Dice: {dice:.2f}\")\n",
        "                ax.axis('off')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(pub_viz_dir, \"prediction_visualization.png\"), dpi=300, bbox_inches='tight')\n",
        "            plt.savefig(os.path.join(pub_viz_dir, \"prediction_visualization.pdf\"), format='pdf', bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "            # Create a metrics table for all samples\n",
        "            all_metrics = []\n",
        "            for i in range(min(20, len(dataset))):  # Get metrics for up to 20 samples\n",
        "                img, mask, _ = dataset[i]\n",
        "                with torch.no_grad():\n",
        "                    pixel_values = processor(images=img.unsqueeze(0), return_tensors=\"pt\")[\"pixel_values\"].to(Config.device)\n",
        "                    logits = model(pixel_values).squeeze()\n",
        "                    pred = (torch.sigmoid(logits) > 0.5).float()\n",
        "                    metrics = calculate_metrics(pred.unsqueeze(0), mask.unsqueeze(0).to(Config.device))\n",
        "                    all_metrics.append(metrics)\n",
        "\n",
        "            # Calculate overall statistics\n",
        "            metrics_df = pd.DataFrame(all_metrics)\n",
        "            metrics_summary = {\n",
        "                'metric': [],\n",
        "                'mean': [],\n",
        "                'std': [],\n",
        "                'min': [],\n",
        "                'max': []\n",
        "            }\n",
        "\n",
        "            for metric in metrics_df.columns:\n",
        "                metrics_summary['metric'].append(metric)\n",
        "                metrics_summary['mean'].append(metrics_df[metric].mean())\n",
        "                metrics_summary['std'].append(metrics_df[metric].std())\n",
        "                metrics_summary['min'].append(metrics_df[metric].min())\n",
        "                metrics_summary['max'].append(metrics_df[metric].max())\n",
        "\n",
        "            summary_df = pd.DataFrame(metrics_summary)\n",
        "            summary_df.to_csv(os.path.join(pub_viz_dir, \"metrics_summary.csv\"), index=False)\n",
        "\n",
        "            # Create a metrics visualization\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            metrics_to_plot = ['iou', 'dice', 'precision', 'recall', 'f1']\n",
        "            means = [summary_df[summary_df['metric'] == m]['mean'].values[0] for m in metrics_to_plot]\n",
        "            stds = [summary_df[summary_df['metric'] == m]['std'].values[0] for m in metrics_to_plot]\n",
        "\n",
        "            x = np.arange(len(metrics_to_plot))\n",
        "            plt.bar(x, means, yerr=stds, align='center', alpha=0.7, capsize=10)\n",
        "            plt.xticks(x, [m.capitalize() for m in metrics_to_plot])\n",
        "            plt.ylabel('Score')\n",
        "            plt.title('Performance Metrics')\n",
        "            plt.ylim(0, 1.0)\n",
        "            plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
        "\n",
        "            for i, v in enumerate(means):\n",
        "                plt.text(i, v + 0.02, f\"{v:.3f}\", ha='center')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(pub_viz_dir, \"metrics_visualization.png\"), dpi=300)\n",
        "            plt.close()\n",
        "\n",
        "            print(f\"Basic visualizations saved to {pub_viz_dir}\")\n",
        "\n",
        "        # Generate publication-quality visualizations\n",
        "        print(\"\\nGenerating publication-ready visualizations...\")\n",
        "\n",
        "        # If we have attention capabilities, use the advanced function\n",
        "        if has_attention:\n",
        "            # Function to generate publication-quality visualizations with attention analysis\n",
        "            def generate_publication_visualizations(model, dataset, processor, save_dir, num_samples=5):\n",
        "                \"\"\"Generate high-quality visualizations for scientific publication\"\"\"\n",
        "                pub_viz_dir = os.path.join(save_dir, \"publication_figures\")\n",
        "                os.makedirs(pub_viz_dir, exist_ok=True)\n",
        "\n",
        "                # Select representative samples\n",
        "                indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
        "\n",
        "                # Create a figure with multiple rows for samples and columns for visualizations\n",
        "                fig = plt.figure(figsize=(15, num_samples * 4))\n",
        "                gs = fig.add_gridspec(num_samples, 4, hspace=0.2, wspace=0.1)\n",
        "\n",
        "                for i, idx in enumerate(indices):\n",
        "                    img, mask, img_path = dataset[idx]\n",
        "                    img_name = os.path.basename(img_path)\n",
        "\n",
        "                    # Process image and get predictions\n",
        "                    model.eval()\n",
        "                    with torch.no_grad():\n",
        "                        pixel_values = processor(images=img.unsqueeze(0), return_tensors=\"pt\")[\"pixel_values\"].to(Config.device)\n",
        "                        outputs = model.model(pixel_values=pixel_values, output_attentions=True)\n",
        "                        logits = model(pixel_values).squeeze()\n",
        "                        pred = (torch.sigmoid(logits) > 0.5).float()\n",
        "\n",
        "                        # Get attention map (CLS token to patches)\n",
        "                        attn = outputs.image_attentions[-1]  # Last layer\n",
        "                        attn = attn.mean(1)[0]  # Average across heads, first batch\n",
        "                        cls_attn = attn[0, 1:].reshape(Config.patch_grid, Config.patch_grid).cpu().numpy()\n",
        "\n",
        "                        # Calculate error map\n",
        "                        error_map = torch.abs(pred - mask.to(Config.device)).cpu().numpy()\n",
        "\n",
        "                    # Original image\n",
        "                    ax = fig.add_subplot(gs[i, 0])\n",
        "                    ax.imshow(img.permute(1, 2, 0).cpu().numpy())\n",
        "                    ax.set_title(\"Input Image\" if i == 0 else None)\n",
        "                    ax.axis('off')\n",
        "\n",
        "                    # Ground truth\n",
        "                    ax = fig.add_subplot(gs[i, 1])\n",
        "                    ax.imshow(mask.cpu().numpy(), cmap='gray')\n",
        "                    ax.set_title(\"Ground Truth\" if i == 0 else None)\n",
        "                    ax.axis('off')\n",
        "\n",
        "                    # Model prediction\n",
        "                    ax = fig.add_subplot(gs[i, 2])\n",
        "                    pred_np = pred.cpu().numpy()\n",
        "                    ax.imshow(pred_np, cmap='gray')\n",
        "                    ax.set_title(\"Model Prediction\" if i == 0 else None)\n",
        "                    ax.axis('off')\n",
        "\n",
        "                    # Attention map\n",
        "                    ax = fig.add_subplot(gs[i, 3])\n",
        "                    im = ax.imshow(cls_attn, cmap='viridis')\n",
        "                    ax.set_title(\"Attention Map\" if i == 0 else None)\n",
        "                    ax.axis('off')\n",
        "\n",
        "                    # Add colorbar to the last attention map\n",
        "                    if i == 0:\n",
        "                        plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(os.path.join(pub_viz_dir, \"multi_sample_visualization.png\"), dpi=300, bbox_inches='tight')\n",
        "                plt.savefig(os.path.join(pub_viz_dir, \"multi_sample_visualization.pdf\"), format='pdf', bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "                # Generate attention concentration analysis across samples\n",
        "                plt.figure(figsize=(10, 6))\n",
        "\n",
        "                attention_concentrations = []\n",
        "                sample_labels = []\n",
        "\n",
        "                for i, idx in enumerate(indices):\n",
        "                    img, mask, img_path = dataset[idx]\n",
        "                    sample_labels.append(f\"Sample {i+1}\")\n",
        "\n",
        "                    # Process image\n",
        "                    model.eval()\n",
        "                    with torch.no_grad():\n",
        "                        pixel_values = processor(images=img.unsqueeze(0), return_tensors=\"pt\")[\"pixel_values\"].to(Config.device)\n",
        "                        outputs = model.model(pixel_values=pixel_values, output_attentions=True)\n",
        "\n",
        "                        # Get attention from last layer\n",
        "                        attn = outputs.image_attentions[-1][0]  # Last layer, first batch\n",
        "\n",
        "                        # For each attention head\n",
        "                        head_concentrations = []\n",
        "                        for head_idx in range(attn.shape[0]):\n",
        "                            # Get CLS token attention distribution\n",
        "                            attn_dist = attn[head_idx, 0, 1:].cpu().numpy()\n",
        "                            # Sort in descending order\n",
        "                            sorted_attn = np.sort(attn_dist)[::-1]\n",
        "                            # Calculate cumulative sum\n",
        "                            cumsum_attn = np.cumsum(sorted_attn) / np.sum(sorted_attn)\n",
        "                            # Find number of patches for 50% attention\n",
        "                            num_patches_50 = np.argmax(cumsum_attn >= 0.5) + 1\n",
        "                            # Calculate concentration\n",
        "                            concentration = num_patches_50 / len(attn_dist)\n",
        "                            head_concentrations.append(concentration)\n",
        "\n",
        "                        attention_concentrations.append(head_concentrations)\n",
        "\n",
        "                # Convert to numpy array for easier manipulation\n",
        "                attention_concentrations = np.array(attention_concentrations)\n",
        "\n",
        "                # Plot boxplot of attention concentration across heads for each sample\n",
        "                plt.boxplot(attention_concentrations.T, labels=sample_labels)\n",
        "                plt.ylabel('Attention Concentration (patches for 50% attention / total patches)')\n",
        "                plt.title('Attention Concentration Analysis Across Samples')\n",
        "                plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "                plt.savefig(os.path.join(pub_viz_dir, \"attention_concentration_analysis.png\"), dpi=300)\n",
        "                plt.close()\n",
        "\n",
        "                # Generate statistics summary\n",
        "                stats_file = os.path.join(pub_viz_dir, \"attention_statistics.txt\")\n",
        "                with open(stats_file, 'w') as f:\n",
        "                    f.write(\"Attention Concentration Statistics\\n\")\n",
        "                    f.write(\"================================\\n\\n\")\n",
        "                    f.write(\"Lower values indicate more focused attention\\n\\n\")\n",
        "\n",
        "                    f.write(\"Per Sample Statistics:\\n\")\n",
        "                    for i, sample in enumerate(sample_labels):\n",
        "                        mean_conc = np.mean(attention_concentrations[i])\n",
        "                        min_conc = np.min(attention_concentrations[i])\n",
        "                        max_conc = np.max(attention_concentrations[i])\n",
        "                        f.write(f\"{sample}: Mean={mean_conc:.4f}, Min={min_conc:.4f}, Max={max_conc:.4f}\\n\")\n",
        "\n",
        "                    f.write(\"\\nOverall Statistics:\\n\")\n",
        "                    overall_mean = np.mean(attention_concentrations)\n",
        "                    overall_std = np.std(attention_concentrations)\n",
        "                    f.write(f\"Mean concentration: {overall_mean:.4f} ± {overall_std:.4f}\\n\")\n",
        "\n",
        "                print(f\"Publication visualizations saved to {pub_viz_dir}\")\n",
        "\n",
        "            try:\n",
        "                # Use the advanced visualization with attention if available\n",
        "                generate_publication_visualizations(model, val_dataset, processor, Config.save_path)\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating advanced visualizations: {e}\")\n",
        "                print(\"Falling back to basic visualizations...\")\n",
        "                generate_basic_visualizations(model, val_dataset, processor, Config.save_path)\n",
        "        else:\n",
        "            # Use the basic visualization if attention is not available\n",
        "            generate_basic_visualizations(model, val_dataset, processor, Config.save_path)\n",
        "\n",
        "        # Generate GradCAM visualizations for interpretability\n",
        "        print(\"\\nGenerating GradCAM visualizations for interpretability...\")\n",
        "        try:\n",
        "            # Create a subset for GradCAM analysis\n",
        "            num_samples = min(10, len(val_dataset))\n",
        "            indices = np.random.choice(len(val_dataset), num_samples, replace=False)\n",
        "\n",
        "            for i, idx in enumerate(indices):\n",
        "                img, mask, img_path = val_dataset[idx]\n",
        "                img_name = os.path.basename(img_path)\n",
        "\n",
        "                # Generate and save GradCAM\n",
        "                gradcam_path = os.path.join(Config.gradcam_dir, f\"gradcam_{img_name}\")\n",
        "                visualize_gradcam(model, img, mask, processor, gradcam_path)\n",
        "\n",
        "            print(f\"GradCAM visualizations saved to {Config.gradcam_dir}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating GradCAM visualizations: {e}\")\n",
        "\n",
        "        # Perform feature embedding analysis\n",
        "        print(\"\\nAnalyzing feature embeddings...\")\n",
        "        try:\n",
        "            embedding_df = analyze_feature_embeddings(model, val_dataset, processor, Config.analysis_dir)\n",
        "            print(f\"Feature embedding analysis saved to {Config.analysis_dir}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error analyzing feature embeddings: {e}\")\n",
        "\n",
        "        # Perform statistical analysis\n",
        "        print(\"\\nPerforming comprehensive statistical analysis...\")\n",
        "        try:\n",
        "            stats = statistical_analysis(model, val_dataset, processor, Config.analysis_dir)\n",
        "            print(f\"Statistical analysis saved to {Config.analysis_dir}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error performing statistical analysis: {e}\")\n",
        "\n",
        "        print(\"\\n✅ All analyses completed successfully!\")\n",
        "        print(f\"  All results saved to: {Config.save_path}\")\n",
        "        print(\"  The enhanced analysis provides:\")\n",
        "        print(\"  - Synthetic multi-scale features for improved segmentation\")\n",
        "        print(\"  - GradCAM visualizations for model interpretability\")\n",
        "        print(\"  - Feature embedding analysis for understanding model behavior\")\n",
        "        print(\"  - Comprehensive statistical analysis by defect class, size, and location\")\n",
        "        print(\"  - Publication-ready visualizations and metrics\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during training: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-X2f22-7Unwb",
        "outputId": "914232a1-b421-4b93-bcc5-169413a8b238"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Found 522 images with masks\n",
            "Training on 417 samples, validating on 105 samples\n",
            "Using FocalDiceLoss\n",
            "WARNING: This FLAVA model doesn't provide attention maps. Attention visualization will be disabled.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1 Training:   0%|          | 0/105 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: Multi-scale features not available in this FLAVA model version.\n",
            "Available outputs: ['image_embeddings', 'image_output']\n",
            "Falling back to single-scale features.\n",
            "Using synthetic multi-scale features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 1 Training:   1%|          | 1/105 [00:01<02:40,  1.55s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 1 Training:  20%|██        | 21/105 [00:06<00:30,  2.77it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 1 Training:  39%|███▉      | 41/105 [00:11<00:27,  2.29it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 1 Training:  58%|█████▊    | 61/105 [00:16<00:15,  2.76it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 1 Training:  77%|███████▋  | 81/105 [00:21<00:08,  2.75it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 1 Training:  96%|█████████▌| 101/105 [00:26<00:01,  2.51it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 1 Training: 100%|██████████| 105/105 [00:27<00:00,  3.82it/s]\n",
            "Epoch 1 Validation: 100%|██████████| 27/27 [00:04<00:00,  6.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  New best model saved! IoU: 0.6077\n",
            "\n",
            "[Epoch 1/10] Summary:\n",
            "  Train - Loss: 0.3981, IoU: 0.4440, Dice: 0.5323\n",
            "  Validation - Loss: 0.3484, IoU: 0.6077, Dice: 0.6656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 2 Training:   0%|          | 0/105 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 2 Training:   1%|          | 1/105 [00:00<01:22,  1.26it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 2 Training:  20%|██        | 21/105 [00:07<00:44,  1.91it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 2 Training:  39%|███▉      | 41/105 [00:13<00:31,  2.06it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 2 Training:  58%|█████▊    | 61/105 [00:20<00:34,  1.28it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 2 Training:  77%|███████▋  | 81/105 [00:27<00:09,  2.48it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 2 Training:  96%|█████████▌| 101/105 [00:33<00:01,  2.47it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 2 Training: 100%|██████████| 105/105 [00:34<00:00,  3.08it/s]\n",
            "Epoch 2 Validation: 100%|██████████| 27/27 [00:04<00:00,  5.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  New best model saved! IoU: 0.6622\n",
            "\n",
            "[Epoch 2/10] Summary:\n",
            "  Train - Loss: 0.3416, IoU: 0.5964, Dice: 0.6603\n",
            "  Validation - Loss: 0.3274, IoU: 0.6622, Dice: 0.7070\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 3 Training:   0%|          | 0/105 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 3 Training:   1%|          | 1/105 [00:00<01:16,  1.36it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 3 Training:  20%|██        | 21/105 [00:21<00:48,  1.72it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 3 Training:  39%|███▉      | 41/105 [00:27<00:27,  2.35it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 3 Training:  58%|█████▊    | 61/105 [00:33<00:29,  1.51it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 3 Training:  77%|███████▋  | 81/105 [00:39<00:10,  2.23it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 3 Training:  96%|█████████▌| 101/105 [00:45<00:01,  2.21it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 3 Training: 100%|██████████| 105/105 [00:46<00:00,  2.27it/s]\n",
            "Epoch 3 Validation: 100%|██████████| 27/27 [00:06<00:00,  4.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  New best model saved! IoU: 0.6786\n",
            "\n",
            "[Epoch 3/10] Summary:\n",
            "  Train - Loss: 0.3176, IoU: 0.6561, Dice: 0.6972\n",
            "  Validation - Loss: 0.3107, IoU: 0.6786, Dice: 0.7162\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 4 Training:   0%|          | 0/105 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 4 Training:   1%|          | 1/105 [00:00<01:23,  1.25it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 4 Training:  20%|██        | 21/105 [00:05<00:31,  2.64it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 4 Training:  39%|███▉      | 41/105 [00:12<00:26,  2.38it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 4 Training:  58%|█████▊    | 61/105 [00:17<00:17,  2.52it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 4 Training:  77%|███████▋  | 81/105 [00:23<00:12,  1.92it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 4 Training:  96%|█████████▌| 101/105 [00:29<00:01,  2.40it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 4 Training: 100%|██████████| 105/105 [00:30<00:00,  3.48it/s]\n",
            "Epoch 4 Validation: 100%|██████████| 27/27 [00:04<00:00,  5.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  New best model saved! IoU: 0.6884\n",
            "\n",
            "[Epoch 4/10] Summary:\n",
            "  Train - Loss: 0.3031, IoU: 0.6884, Dice: 0.7167\n",
            "  Validation - Loss: 0.3137, IoU: 0.6884, Dice: 0.7178\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 5 Training:   0%|          | 0/105 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 5 Training:   1%|          | 1/105 [00:01<02:44,  1.58s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 5 Training:  20%|██        | 21/105 [00:06<00:30,  2.72it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 5 Training:  39%|███▉      | 41/105 [00:12<00:31,  2.05it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 5 Training:  58%|█████▊    | 61/105 [00:17<00:17,  2.48it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 5 Training:  77%|███████▋  | 81/105 [00:24<00:14,  1.64it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 5 Training:  96%|█████████▌| 101/105 [00:29<00:01,  2.50it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 5 Training: 100%|██████████| 105/105 [00:30<00:00,  3.44it/s]\n",
            "Epoch 5 Validation: 100%|██████████| 27/27 [00:04<00:00,  5.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  New best model saved! IoU: 0.6950\n",
            "\n",
            "[Epoch 5/10] Summary:\n",
            "  Train - Loss: 0.2927, IoU: 0.7089, Dice: 0.7389\n",
            "  Validation - Loss: 0.3068, IoU: 0.6950, Dice: 0.7278\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 6 Training:   0%|          | 0/105 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 6 Training:   1%|          | 1/105 [00:16<28:37, 16.51s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 6 Training:  20%|██        | 21/105 [00:21<00:32,  2.62it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 6 Training:  39%|███▉      | 41/105 [00:27<00:33,  1.94it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 6 Training:  58%|█████▊    | 61/105 [00:33<00:18,  2.39it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 6 Training:  77%|███████▋  | 81/105 [00:38<00:10,  2.33it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 6 Training:  96%|█████████▌| 101/105 [00:44<00:01,  2.56it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 6 Training: 100%|██████████| 105/105 [00:44<00:00,  2.34it/s]\n",
            "Epoch 6 Validation: 100%|██████████| 27/27 [00:04<00:00,  5.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  New best model saved! IoU: 0.6980\n",
            "\n",
            "[Epoch 6/10] Summary:\n",
            "  Train - Loss: 0.2832, IoU: 0.7287, Dice: 0.7481\n",
            "  Validation - Loss: 0.3162, IoU: 0.6980, Dice: 0.7343\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 7 Training:   0%|          | 0/105 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 7 Training:   1%|          | 1/105 [00:01<01:48,  1.04s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 7 Training:  20%|██        | 21/105 [00:07<00:53,  1.58it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 7 Training:  39%|███▉      | 41/105 [00:11<00:23,  2.70it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 7 Training:  58%|█████▊    | 61/105 [00:17<00:19,  2.31it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 7 Training:  77%|███████▋  | 81/105 [00:22<00:08,  2.80it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 7 Training:  96%|█████████▌| 101/105 [00:27<00:01,  2.64it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 7 Training: 100%|██████████| 105/105 [00:28<00:00,  3.74it/s]\n",
            "Epoch 7 Validation: 100%|██████████| 27/27 [00:04<00:00,  5.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  New best model saved! IoU: 0.7063\n",
            "\n",
            "[Epoch 7/10] Summary:\n",
            "  Train - Loss: 0.2769, IoU: 0.7276, Dice: 0.7570\n",
            "  Validation - Loss: 0.3029, IoU: 0.7063, Dice: 0.7433\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 8 Training:   0%|          | 0/105 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 8 Training:   1%|          | 1/105 [00:00<01:16,  1.36it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 8 Training:  20%|██        | 21/105 [00:06<00:36,  2.30it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 8 Training:  39%|███▉      | 41/105 [00:10<00:22,  2.82it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 8 Training:  58%|█████▊    | 61/105 [00:15<00:15,  2.79it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 8 Training:  77%|███████▋  | 81/105 [00:20<00:08,  2.72it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 8 Training:  96%|█████████▌| 101/105 [00:25<00:01,  2.87it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 8 Training: 100%|██████████| 105/105 [00:26<00:00,  3.97it/s]\n",
            "Epoch 8 Validation: 100%|██████████| 27/27 [00:04<00:00,  5.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  New best model saved! IoU: 0.7103\n",
            "\n",
            "[Epoch 8/10] Summary:\n",
            "  Train - Loss: 0.2732, IoU: 0.7409, Dice: 0.7625\n",
            "  Validation - Loss: 0.2988, IoU: 0.7103, Dice: 0.7384\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 9 Training:   0%|          | 0/105 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 9 Training:   1%|          | 1/105 [00:00<01:22,  1.27it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 9 Training:  20%|██        | 21/105 [00:05<00:29,  2.80it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 9 Training:  39%|███▉      | 41/105 [00:10<00:29,  2.17it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 9 Training:  58%|█████▊    | 61/105 [00:15<00:16,  2.73it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 9 Training:  77%|███████▋  | 81/105 [00:20<00:08,  2.80it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 9 Training:  96%|█████████▌| 101/105 [00:26<00:01,  2.60it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 9 Training: 100%|██████████| 105/105 [00:26<00:00,  3.90it/s]\n",
            "Epoch 9 Validation: 100%|██████████| 27/27 [00:05<00:00,  5.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Epoch 9/10] Summary:\n",
            "  Train - Loss: 0.2660, IoU: 0.7498, Dice: 0.7663\n",
            "  Validation - Loss: 0.2794, IoU: 0.7086, Dice: 0.7360\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 10 Training:   0%|          | 0/105 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 10 Training:   1%|          | 1/105 [00:01<02:12,  1.27s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 10 Training:  20%|██        | 21/105 [00:06<00:29,  2.81it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 10 Training:  39%|███▉      | 41/105 [00:11<00:23,  2.73it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 10 Training:  58%|█████▊    | 61/105 [00:16<00:20,  2.17it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 10 Training:  77%|███████▋  | 81/105 [00:21<00:08,  2.75it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 10 Training:  96%|█████████▌| 101/105 [00:26<00:01,  2.72it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "Epoch 10 Training: 100%|██████████| 105/105 [00:27<00:00,  3.87it/s]\n",
            "Epoch 10 Validation: 100%|██████████| 27/27 [00:05<00:00,  5.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  New best model saved! IoU: 0.7153\n",
            "\n",
            "[Epoch 10/10] Summary:\n",
            "  Train - Loss: 0.2635, IoU: 0.7477, Dice: 0.7673\n",
            "  Validation - Loss: 0.2956, IoU: 0.7153, Dice: 0.7385\n",
            "\n",
            "✅ Model training completed! Results saved to: /content/drive/MyDrive/new_flava/attention_seg_head_enhanced\n",
            "  Best validation IoU: 0.7153 (Epoch 10)\n",
            "  Final validation IoU: 0.7153\n",
            "  Best model saved to: /content/drive/MyDrive/new_flava/attention_seg_head_enhanced/flava_seg_head_enhanced_best.pth\n",
            "Found 522 images with masks\n",
            "\n",
            "Generating publication-ready visualizations...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-3203514813>:1288: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
            "  plt.tight_layout()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Basic visualizations saved to /content/drive/MyDrive/new_flava/attention_seg_head_enhanced/publication_figures\n",
            "\n",
            "Generating GradCAM visualizations for interpretability...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GradCAM visualizations saved to /content/drive/MyDrive/new_flava/attention_seg_head_enhanced/gradcam\n",
            "\n",
            "Analyzing feature embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting embeddings:   0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:1614: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Extracting embeddings:   0%|          | 0/100 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error analyzing feature embeddings: Wrong shape for input_ids (shape torch.Size([1, 3, 224, 224])) or attention_mask (shape torch.Size([1, 3, 224, 224]))\n",
            "\n",
            "Performing comprehensive statistical analysis...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing statistical analysis: 100%|██████████| 105/105 [00:03<00:00, 28.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Statistical analysis saved to /content/drive/MyDrive/new_flava/attention_seg_head_enhanced/advanced_analysis\n",
            "\n",
            "✅ All analyses completed successfully!\n",
            "  All results saved to: /content/drive/MyDrive/new_flava/attention_seg_head_enhanced\n",
            "  The enhanced analysis provides:\n",
            "  - Synthetic multi-scale features for improved segmentation\n",
            "  - GradCAM visualizations for model interpretability\n",
            "  - Feature embedding analysis for understanding model behavior\n",
            "  - Comprehensive statistical analysis by defect class, size, and location\n",
            "  - Publication-ready visualizations and metrics\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gVAwOuoTeKHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import shutil\n",
        "from transformers import FlavaModel, FlavaProcessor\n",
        "\n",
        "# مسیرها و تنظیمات - مطمئن شوید با تنظیمات شما مطابقت دارد\n",
        "save_path = \"/content/drive/MyDrive/new_flava/attention_seg_head_enhanced\"\n",
        "base_model_path = \"/content/drive/MyDrive//flava_finetuned\"\n",
        "best_epoch = 10  # بهترین epoch که از پیام قبلی مشخص شده است\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# تعریف کلاس مدل - مطمئن شوید با همان کلاسی که برای آموزش استفاده کردید مطابقت دارد\n",
        "class FLAVASegmenter(nn.Module):\n",
        "    def __init__(self, base_model_path):\n",
        "        super().__init__()\n",
        "        self.model = FlavaModel.from_pretrained(base_model_path)\n",
        "        # Extract features from multiple transformer layers\n",
        "        self.use_multi_scale = Config.use_multi_scale\n",
        "\n",
        "        # Add projection layers for each scale\n",
        "        self.projections = nn.ModuleList([\n",
        "            nn.Linear(self.model.config.hidden_size, 256)\n",
        "            for _ in range(4)  # Use last 4 layers\n",
        "        ])\n",
        "\n",
        "        # Fusion layer for multi-scale features\n",
        "        self.fusion = nn.Conv2d(256*4, 256, kernel_size=1) if self.use_multi_scale else None\n",
        "\n",
        "        # For synthetic multi-scale if real ones not available\n",
        "        if Config.synthetic_multi_scale:\n",
        "            # Projection for synthetic multi-scale features\n",
        "            self.syn_projection = nn.Linear(self.model.config.hidden_size, 256)\n",
        "            # Fusion for synthetic features (3 scales: original, 2x2 pooled, 4x4 pooled)\n",
        "            self.syn_fusion = nn.Conv2d(256*3, 256, kernel_size=1)\n",
        "\n",
        "        # Segmentation head with convolutional layers\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 1, kernel_size=1)\n",
        "        )\n",
        "\n",
        "        # Store activation maps for GradCAM\n",
        "        self.activation = {}\n",
        "        self.gradients = {}\n",
        "        self._register_hooks()\n",
        "\n",
        "        # Flag to indicate if we had to fall back to single-scale\n",
        "        self.using_fallback = False\n",
        "        self.using_synthetic = False\n",
        "\n",
        "    def _register_hooks(self):\n",
        "        # Register hooks for GradCAM\n",
        "        def get_activation(name):\n",
        "            def hook(module, input, output):\n",
        "                self.activation[name] = output\n",
        "            return hook\n",
        "\n",
        "        def get_gradient(name):\n",
        "            def hook(module, grad_in, grad_out):\n",
        "                self.gradients[name] = grad_out[0]\n",
        "            return hook\n",
        "\n",
        "        # Register hooks for the last convolutional layer\n",
        "        self.head[0].register_forward_hook(get_activation('conv_feature'))\n",
        "        self.head[0].register_backward_hook(get_gradient('conv_feature'))\n",
        "\n",
        "    def create_synthetic_multi_scale(self, embeddings):\n",
        "        \"\"\"Create synthetic multi-scale features from a single embedding layer\"\"\"\n",
        "        # Original scale\n",
        "        b, n, c = embeddings.shape\n",
        "\n",
        "        # Project embeddings\n",
        "        projected = self.syn_projection(embeddings)\n",
        "\n",
        "        # Reshape to spatial dimensions (excluding CLS token)\n",
        "        spatial = projected.reshape(b, Config.patch_grid, Config.patch_grid, -1).permute(0, 3, 1, 2)\n",
        "\n",
        "        # Create multi-scale features\n",
        "        features = [spatial]  # Original scale\n",
        "\n",
        "        # Scale 2: Pooled 2x2\n",
        "        pooled2 = F.avg_pool2d(spatial, kernel_size=2, stride=2)\n",
        "        upsampled2 = F.interpolate(pooled2, size=(Config.patch_grid, Config.patch_grid), mode='bilinear', align_corners=False)\n",
        "        features.append(upsampled2)\n",
        "\n",
        "        # Scale 3: Pooled 4x4\n",
        "        pooled4 = F.avg_pool2d(spatial, kernel_size=4, stride=4)\n",
        "        upsampled4 = F.interpolate(pooled4, size=(Config.patch_grid, Config.patch_grid), mode='bilinear', align_corners=False)\n",
        "        features.append(upsampled4)\n",
        "\n",
        "        # Concatenate along channel dimension\n",
        "        multi_scale = torch.cat(features, dim=1)\n",
        "\n",
        "        # Fuse multi-scale features\n",
        "        fused = self.syn_fusion(multi_scale)\n",
        "\n",
        "        return fused\n",
        "\n",
        "    def forward(self, pixel_inputs):\n",
        "        # Get outputs with attention (for visualization)\n",
        "        outputs = self.model(\n",
        "            pixel_values=pixel_inputs,\n",
        "            output_hidden_states=self.use_multi_scale,\n",
        "            output_attentions=True\n",
        "        )\n",
        "\n",
        "        # Store attention maps for visualization if available\n",
        "        self.last_attentions = outputs.image_attentions if hasattr(outputs, 'image_attentions') else None\n",
        "\n",
        "        # Check if multi-scale is actually available in this model version\n",
        "        multi_scale_available = hasattr(outputs, 'image_hidden_states') and outputs.image_hidden_states is not None\n",
        "\n",
        "        # Log the first time we have to fall back\n",
        "        if self.use_multi_scale and not multi_scale_available and not self.using_fallback:\n",
        "            print(\"WARNING: Multi-scale features not available in this FLAVA model version.\")\n",
        "            print(\"Available outputs:\", list(outputs.keys()))\n",
        "            print(\"Falling back to single-scale features.\")\n",
        "            self.using_fallback = True\n",
        "\n",
        "        if self.use_multi_scale and multi_scale_available:\n",
        "            # Use last 4 layers\n",
        "            hidden_states = outputs.image_hidden_states[-4:]\n",
        "            multi_scale_features = []\n",
        "\n",
        "            for i, hidden_state in enumerate(hidden_states):\n",
        "                # Skip CLS token\n",
        "                patches = hidden_state[:, 1:, :]\n",
        "                b, n, c = patches.shape\n",
        "                # Project and reshape to spatial dimensions\n",
        "                projected = self.projections[i](patches)\n",
        "                spatial = projected.reshape(b, Config.patch_grid, Config.patch_grid, -1).permute(0, 3, 1, 2)\n",
        "                multi_scale_features.append(spatial)\n",
        "\n",
        "            # Concatenate features along channel dimension\n",
        "            fused_features = torch.cat(multi_scale_features, dim=1)\n",
        "            # Fuse multi-scale features\n",
        "            fused_features = self.fusion(fused_features)\n",
        "            # Apply segmentation head\n",
        "            seg_logits = self.head(fused_features)\n",
        "\n",
        "        elif Config.synthetic_multi_scale and not self.using_synthetic:\n",
        "            # Use synthetic multi-scale features\n",
        "            print(\"Using synthetic multi-scale features\")\n",
        "            self.using_synthetic = True\n",
        "\n",
        "            # Get image embeddings\n",
        "            patches = outputs.image_embeddings[:, 1:, :]  # Skip CLS token\n",
        "\n",
        "            # Create synthetic multi-scale features\n",
        "            fused_features = self.create_synthetic_multi_scale(patches)\n",
        "\n",
        "            # Apply segmentation head\n",
        "            seg_logits = self.head(fused_features)\n",
        "\n",
        "        else:\n",
        "            # Single-scale approach using image embeddings\n",
        "            patches = outputs.image_embeddings[:, 1:, :]  # Skip CLS token\n",
        "            b, n, c = patches.shape\n",
        "            projected = self.projections[0](patches)\n",
        "            spatial = projected.reshape(b, Config.patch_grid, Config.patch_grid, -1).permute(0, 3, 1, 2)\n",
        "            seg_logits = self.head(spatial)\n",
        "\n",
        "        return seg_logits\n",
        "\n",
        "    def get_gradcam(self, target_layer='conv_feature'):\n",
        "        \"\"\"Generate GradCAM heatmap for interpretability\"\"\"\n",
        "        if target_layer not in self.activation or target_layer not in self.gradients:\n",
        "            print(f\"Warning: {target_layer} activations or gradients not found\")\n",
        "            return None\n",
        "\n",
        "        # Get activations and gradients for the target layer\n",
        "        activations = self.activation[target_layer]\n",
        "        gradients = self.gradients[target_layer]\n",
        "\n",
        "        # Global average pooling of gradients\n",
        "        weights = torch.mean(gradients, dim=(2, 3), keepdim=True)\n",
        "\n",
        "        # Weighted sum of activation maps\n",
        "        cam = torch.sum(weights * activations, dim=1, keepdim=True)\n",
        "\n",
        "        # Apply ReLU to focus on features that have a positive influence\n",
        "        cam = F.relu(cam)\n",
        "\n",
        "        # Normalize\n",
        "        if torch.max(cam) > 0:\n",
        "            cam = cam / torch.max(cam)\n",
        "\n",
        "        return cam\n",
        "\n",
        "# ایجاد مدل خالی\n",
        "model = FLAVASegmenter(base_model_path).to(device)\n",
        "\n",
        "# بارگذاری بهترین checkpoint\n",
        "print(f\"در حال بارگذاری بهترین مدل از epoch {best_epoch}...\")\n",
        "best_checkpoint_path = os.path.join(save_path, f\"model_checkpoint_epoch{best_epoch}.pth\")\n",
        "\n",
        "# بارگذاری با weights_only=False برای جلوگیری از خطای PyTorch 2.6\n",
        "checkpoint = torch.load(best_checkpoint_path, weights_only=False)\n",
        "\n",
        "# بارگذاری وزن‌های مدل\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "print(\"وزن‌های مدل با موفقیت بارگذاری شدند!\")\n",
        "\n",
        "# ذخیره به عنوان بهترین مدل\n",
        "best_model_path = os.path.join(save_path, \"best_model.pth\")\n",
        "torch.save({\n",
        "    'epoch': best_epoch,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': checkpoint['optimizer_state_dict'] if 'optimizer_state_dict' in checkpoint else None,\n",
        "    'val_iou': checkpoint['metrics']['val_iou'][best_epoch-1] if 'metrics' in checkpoint and 'val_iou' in checkpoint['metrics'] else None,\n",
        "    'val_dice': checkpoint['metrics']['val_dice'][best_epoch-1] if 'metrics' in checkpoint and 'val_dice' in checkpoint['metrics'] else None,\n",
        "}, best_model_path)\n",
        "print(f\"بهترین مدل در مسیر زیر ذخیره شد: {best_model_path}\")\n",
        "\n",
        "# ایجاد کپی از بهترین checkpoint به عنوان مرجع\n",
        "best_link_path = os.path.join(save_path, \"best_checkpoint.pth\")\n",
        "if os.path.exists(best_link_path):\n",
        "    os.remove(best_link_path)\n",
        "shutil.copy2(best_checkpoint_path, best_link_path)\n",
        "print(f\"کپی بهترین checkpoint در مسیر زیر ایجاد شد: {best_link_path}\")\n",
        "\n",
        "# اکنون می‌توانید مدل را برای استفاده‌های بعدی ذخیره کنید\n",
        "model_only_path = os.path.join(save_path, \"best_model_weights_only.pth\")\n",
        "torch.save(model.state_dict(), model_only_path)\n",
        "print(f\"وزن‌های بهترین مدل به صورت جداگانه در مسیر زیر ذخیره شدند: {model_only_path}\")\n",
        "\n",
        "print(\"\\nتمام عملیات با موفقیت انجام شد!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3-w-Urye5-u",
        "outputId": "229a55be-5a35-45ca-875c-e1c8df24be23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "در حال بارگذاری بهترین مدل از epoch 10...\n",
            "وزن‌های مدل با موفقیت بارگذاری شدند!\n",
            "بهترین مدل در مسیر زیر ذخیره شد: /content/drive/MyDrive/new_flava/attention_seg_head_enhanced/best_model.pth\n",
            "کپی بهترین checkpoint در مسیر زیر ایجاد شد: /content/drive/MyDrive/new_flava/attention_seg_head_enhanced/best_checkpoint.pth\n",
            "وزن‌های بهترین مدل به صورت جداگانه در مسیر زیر ذخیره شدند: /content/drive/MyDrive/new_flava/attention_seg_head_enhanced/best_model_weights_only.pth\n",
            "\n",
            "تمام عملیات با موفقیت انجام شد!\n"
          ]
        }
      ]
    }
  ]
}